---
{"dg-publish":true,"permalink":"/03-大模型/Dropout 到底是如何工作的？/","title":"Dropout 到底是如何工作的？"}
---


# Dropout 到底是如何工作的？

原文：[大多数人并不完全理解 Dropout 是如何运作的 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/680112242)

![大多数人并不完全理解 Dropout 是如何运作的](https://pic1.zhimg.com/v2-bf54b572bbb1992a6fb99d6d23c651ef_720w.jpg?source=7e7ef6e2)

我在面试算法岗位候选人的时候，比较喜欢问应用 ML 方面的问题。

然而，也有一些概念性的问题，比如下面这个，我就经常爱问：

_Dropout 是如何运作的？_

很简单，对吧？

显然，每个候选人都会给我一个不完整的答案，我将在下面提到：

---

## 候选人的回答

简言之，Dropout 就是将神经网络中的神经元随机归零。这样做是为了规范网络。

![](https://pic4.zhimg.com/v2-23a17fe6bc96d41e313848c332171e2f_b.jpg)

Dropout 将网络中的神经元随机归零

Dropout 仅在训练期间应用，哪些神经元激活归零是使用伯努利分布决定的：

![](https://pic1.zhimg.com/v2-10750757a29028ef8caf4113bf3446dc_b.jpg)

p 是随机归零的概率

---

我的后续问题：_好的，完美！我们在 Dropout 中还会做其他的动作吗？_

候选人：_不，就是这样。我们只将神经元归零并像平常一样训练网络。_

---

当然，我不能说上述回答不正确。他们其实是正确的。

然而，**这只是 Dropout 工作原理的 50%，** 令人失望的是，大多数资源并没有涵盖剩下的 50%。

如果您也只知道我上面提到的 50% 细节，那么请继续阅读，因为有新信息可供您使用。

---

## Dropout 实际上是如何运作的？

首先，我们必须注意 Dropout 仅在训练期间应用，而不在推理/评估阶段应用：

![](https://pic4.zhimg.com/v2-f18b061dc486e318aa408d402b95187f_b.jpg)

dropout 只在训练阶段使用

现在，考虑一个神经元的输入是使用前一个隐藏层中的 100 个神经元计算的：

![](https://pic1.zhimg.com/v2-2c33f735a4348070283d3d527120c834_b.jpg)

一个神经元的输入是前一个隐藏层的多个神经元所计算

为了简单起见，我们假设一些事情：

- 每个黄色神经元激活后的值为 1。
- 从黄色神经元到蓝色神经元的边权重也是 1。

![](https://pic2.zhimg.com/v2-a8bfb82d9827d2af761e92b2d02df52b_b.jpg)

一个合理的假设

结果，蓝色神经元收到的输入将为 100，如下所示：

![](https://pic4.zhimg.com/v2-2525e4d7c536f93fb3c28b2069fdf125_b.jpg)

当不存在 dropout 时，蓝色神经元是所有黄色神经元的输入

好，现在，在训练过程中，如果我们使用了 Dropout，例如 40% 的 dropout 率，那么大约 40% 的黄色神经元激活将被清零。

结果，蓝色神经元收到的输入约为 60，如下所示：

![](https://pic3.zhimg.com/v2-42d4df53028285d90b5a54bc11e42e36_b.jpg)

采用了 Dropout 后的输入

不过，以上观点只在训练阶段。

如果在推理阶段存在相同的场景，那么蓝色神经元收到的输入将为 100。

因此，在相似条件下：

- 训练起见收到的输入：60。
- 推理期间收到的输入：100。

问题来了：

**在训练期间，平均神经元输入明显低于推理期间收到的输入。**

![](https://pic2.zhimg.com/v2-fe98e56c79fa4eaf07c0d6f2f3bc25e1_b.jpg)

更正式地说，使用 Dropout 会显著影响激活的规模。

然而，我们希望整个模型中的神经元在训练和推理期间必须接受大致相同的激活平均值（或期望值）。

**为了解决这个问题，Dropout 执行了一个额外的步骤。**

这个想法是在训练期间**缩放剩余的主动输入。**

最简单的方法是将训练期间的所有激活缩放一个因子 1/(1-p)，其中 p 是 dropout 率。

例如，在神经元输入 60 上使用此技术，我们将得到以下结果（假设我们的 dropout 率为 40%）：

![](https://pica.zhimg.com/v2-a2b27d0399f8148c8b1b4e8b2a856e30_b.jpg)

缩放因子后，训练和推理的结果就相同了

如上所述，缩放神经元输入使其达到所需的范围，这使得网络的训练和推理阶段保持一致。

---

## 实验验证

事实上，我们可以使用 PyTorch 来验证 Dropout 确实执行了此步骤。

让我们定义一个 dropout 层，如下所示：

```python
import torch
import torch.nn as nn

dropout_layer = nn.Dropout(0.2)
```

现在，我们考虑一个随机张量并将该 dropout 层应用于它：

```python
dropout_layer.train()

x=torch.rand((1, 5))
# tensor([[0.94, 0.13, 0.93, 0.59, 0.86]])

dropout_layer(x)
# tensor([[0.00, 0.16, 1.16, 0.74, 1.08]])
```

如上所述，保留值已增加。

- 第二个值从 0.13->0.16 。
- 第三个值从 0.93->1.16 。
- 等等

更重要的是，保留的值于我们通过显式缩放输入张量获得的值完全相同：

```python
droupout_layer(x)
# tensor([[0.00, 0.16, 1.16, 0.74, 1.08]])

x/(1-p)
# tensor([[0.94, 0.16, 1.16, 0.74, 1.08]])
```

如果我们在评估模式下做同样的事情，我们会注意到没有值被丢弃，也没有发生缩放，这是有道理的，因为 Dropout 仅发生在训练期间：

```python
dropout_layer.eval()
x
# tensor([[0.94, 0.13, 0.93, 0.59, 0.86]])

dropout_layer(x)
# tensor([[0.94, 0.13, 0.93, 0.59, 0.86]])
```

---

这是剩下的 50% 细节，根据我的经验，大多数资源都没有涵盖这些细节，因此大多数人都不知道。

但这是 Dropout 中非常重要的一步，它保持了训练和推理阶段之间的数值一致性。
