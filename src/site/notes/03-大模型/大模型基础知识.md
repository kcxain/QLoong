---
{"dg-publish":true,"permalink":"/03-大模型/大模型基础知识/","title":"大模型基础知识"}
---


# 大模型基础知识

## 历史

### Word2Vec

- [[00-课程笔记/dl-nlp/Note01-Introduction_SVD_and_Word2Vec\|Note01-Introduction_SVD_and_Word2Vec]]

### 反向传播

- [[00-课程笔记/深度学习框架\|深度学习框架]]

### Transformer

- [[03-大模型/手撕多头注意力\|手撕多头注意力]]
- [[03-大模型/位置编码\|位置编码]]

### BERT

- [[01-文章/2022/BERT 原理与代码解析\|BERT 原理与代码解析]]

### 对比学习

## Tricks

- [[03-大模型/Dropout 到底是如何工作的？\|Dropout 到底是如何工作的？]]

## 现在

### LLaMA

- [[03-大模型/LLaMA代码解读\|LLaMA代码解读]]

#### 如何解决大模型重复生成问题？

1. Beam search。比如在 LLAMA 和 NLLB 的原始 Transformers 代码中，beam search 是默认为 false，如果打开并将 seeds 设置为 2，那整体的重复生成情况能减少到原来的 40%；还有一种是基于对比搜索的方式 contrastive sampling [1,2] ，据说是能够达到与人类匹配的水平。但是我试过，在 LLAMA 和 BLOOMZ 上效果巨差；实际上论文里使用 GPT2-large 的生成概率分布和后面两个模型的差异也很大，也就是这种方法不具有一般性。
2. 另一种比较实际的操作，这个其实是从 predict softmax 分布上发现的，一个词重复生成，也就是说下一个位置它的概率值依然是最大的，那么你可以先对前面的句子做 n-gram 的重复检测，对出现重复的词在 next prediction 时做 mask，强行控制模型不选前面 t-n~t-1 出现的重复词时，重复性大大减少，翻译的准确率也会明显提升！就是这么简单粗暴，其实原理也类似于 Reward，强制模型朝着信息熵高的方向来生成。
3. 另外，针对<HJIKL, HJIKLL, HJIKLL..>的分布几乎相同的各向异性，就是 L 的分布与前面 t-1 词的分布趋同，可以认为是 L 这个词的 input embedding 并没有学好！最简单的解决方式，先在通用数据上大批量跑出来哪些词时容易重复的，再对他们组成的词汇单独的 pre 训练，强行改变他们的分布，也能在不降低生成质量的情况下，大幅度改善重复的问题。
4. 还有一个很 trick 的方法，也就是对容易出现重复生成的文本，做对应词语 L 的缺词文本构造，生成多个扩充文本放到训练集里训练。这个无论是在翻译、QA 还是对话里都比较有效，毕竟数据为王嘛，就是让模型对 semi-distinctive 的样本多多学习下。

### LoRA & P-tuning

### Deepspeed

## 实战

### 小米

#### 八股

- 梯度消失、爆炸
- Adam 优化器 [[00-课程笔记/dlsys/深度模型中的优化算法\|深度模型中的优化算法]]
- LLaMA

#### 算法

- 合并有序链表
- 第 k 大数 [[02-算法刷题/关于快排板子\|关于快排板子]]
- 非递归先序遍历
- 无重复的子串
- 最长公共子序列
- 洗牌算法

一面：

问简历上的两个项目，对比学习有哪些 loss，为什么要加温度系数，transformer 结构，Bert 结构，Bert 参数量计算

算法题：力扣 121，122 买卖股票的最佳时机

二面：

重点挖掘其中一个项目，sentence-bert 原理，训练方法，roberta 和 bert 的区别，BPE 和 WordPiece 分词的区别，基于子词分词的优缺点

算法题：行列分别递增的矩阵中查找目标值
